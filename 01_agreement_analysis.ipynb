{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb5f60c-80e0-4a41-b71c-e38c5d33bb6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AGREEMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db579451-9559-46c4-9339-768d33b1d5be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialise packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226bff1b-7426-485e-80ea-8f20c4a9a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bec635-e271-4ebb-9b82-e9b8624095a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import experimental results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1225888-11fa-4159-907e-2a9c71566b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = dict()\n",
    "\n",
    "for group in [\"g1\",\"g2\",\"g3\"]:\n",
    "    annotations[group] = pd.read_csv(f\"./0_data/annotations_{group}.csv\")\n",
    "\n",
    "# create combinations of any two groups\n",
    "for key in [(\"g1\",\"g2\"), (\"g2\",\"g3\"), (\"g1\",\"g3\")]:\n",
    "    \n",
    "    annotations[key] = annotations[key[0]].copy()\n",
    "    annotations[key].columns = annotations[key].columns.str.replace(\"label\", f\"{key[0]}_label\")\n",
    "    \n",
    "    target = annotations[key[1]].copy()\n",
    "    target.columns = target.columns.str.replace(\"label\", f\"{key[1]}_label\")\n",
    "    \n",
    "    annotations[key] = annotations[key].merge(target, on=[\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fc222-e077-4336-a26d-34f0bd4e255b",
   "metadata": {},
   "source": [
    "### Compute inter-annotator agreement metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3a0a4-cdab-4d2d-b54b-691bbd1367e5",
   "metadata": {},
   "source": [
    "**Percentage agreement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d68ce1-33c6-4df1-b623-acb1a456a09e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage agreement in g1: 73.90%\n",
      "Average percentage agreement in g2: 93.72%\n",
      "Average percentage agreement in g3: 72.50%\n",
      "Average percentage agreement in ('g1', 'g2'): 79.04%\n",
      "Average percentage agreement in ('g2', 'g3'): 78.61%\n",
      "Average percentage agreement in ('g1', 'g3'): 72.90%\n"
     ]
    }
   ],
   "source": [
    "def count_votes(row, label):\n",
    "    l = list(row.values[2:])\n",
    "    return l.count(label)\n",
    "\n",
    "def maj_label(row):\n",
    "    if row.n_hateful >= row.n_nonhateful:\n",
    "        return \"Hateful\"\n",
    "    else:\n",
    "        return \"Non-hateful\"\n",
    "\n",
    "def pct_agreement(row):\n",
    "    return max(row.n_hateful, row.n_nonhateful)/(row.n_hateful+row.n_nonhateful)\n",
    "\n",
    "for g in annotations:\n",
    "    annotations[g][\"n_hateful\"]=annotations[g].apply(lambda x: count_votes(x, \"Hateful\"), axis=1)\n",
    "    annotations[g][\"n_nonhateful\"]=annotations[g].apply(lambda x: count_votes(x, \"Non-hateful\"), axis=1)\n",
    "    annotations[g][\"label_maj\"]=annotations[g].apply(lambda x: maj_label(x), axis=1)\n",
    "    annotations[g]['pct_agreement'] = annotations[g].apply(lambda x: pct_agreement(x), axis=1)\n",
    "    print('Average percentage agreement in {}: {:.2%}'.format(g, annotations[g].pct_agreement.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e93f5c-a1ef-45ce-b9c0-e23da775c098",
   "metadata": {},
   "source": [
    "**Fleiss' Kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d194069-7adc-4d88-b11e-76d0d676c9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa in g1: 0.20\n",
      "Fleiss' Kappa in g2: 0.78\n",
      "Fleiss' Kappa in g3: 0.15\n",
      "Fleiss' Kappa in ('g1', 'g2'): 0.36\n",
      "Fleiss' Kappa in ('g2', 'g3'): 0.34\n",
      "Fleiss' Kappa in ('g1', 'g3'): 0.18\n"
     ]
    }
   ],
   "source": [
    "for g in annotations:\n",
    "    vote_matrix = annotations[g][['n_hateful','n_nonhateful']].to_numpy()\n",
    "    \n",
    "    print('Fleiss\\' Kappa in {}: {:.2f}'.format(g,fleiss_kappa(vote_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cfb9aa-895d-47f4-b565-3545ae74e95c",
   "metadata": {},
   "source": [
    "### Compute bootstrap standard errors for agreement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98cd8423-bce2-4bd1-93fa-b07a9c495fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_se(df, runs):\n",
    "    \n",
    "    results_dict={}\n",
    "    \n",
    "    # MEANS\n",
    "    \n",
    "    # RAW AGREEMENT\n",
    "    results_dict['mean_raw_agreement']={}\n",
    "    results_dict['mean_raw_agreement']['abs value'] = df['pct_agreement'].mean()\n",
    "    \n",
    "    # FLEISS KAPPA\n",
    "    results_dict['fleiss_kappa']={}\n",
    "    results_dict['fleiss_kappa']['abs value'] = fleiss_kappa(df[['n_hateful','n_nonhateful']].to_numpy())\n",
    "    \n",
    "    \n",
    "    # BOOTSTRAP\n",
    "    \n",
    "    metric_list = []\n",
    "    for i in range(runs):\n",
    "        b_df = df.sample(frac=1, replace=True)\n",
    "        metric_list.append(b_df['pct_agreement'].mean())\n",
    "    \n",
    "    results_dict['mean_raw_agreement']['bootstrap_sd'] = statistics.pstdev(metric_list)\n",
    "    results_dict['mean_raw_agreement']['bootstrap_ci_995'] = sorted(metric_list)[round(runs*0.995)]\n",
    "    results_dict['mean_raw_agreement']['bootstrap_ci_005'] = sorted(metric_list)[round(runs*0.005)]\n",
    "    \n",
    "    \n",
    "    metric_list = []\n",
    "    for i in range(runs):\n",
    "        b_df = df.sample(frac=1, replace=True)\n",
    "        metric_list.append(fleiss_kappa(b_df[['n_hateful','n_nonhateful']].to_numpy()))\n",
    "    \n",
    "    results_dict['fleiss_kappa']['bootstrap_sd'] = statistics.pstdev(metric_list)\n",
    "    results_dict['fleiss_kappa']['bootstrap_ci_995'] = sorted(metric_list)[round(runs*0.995)]\n",
    "    results_dict['fleiss_kappa']['bootstrap_ci_005'] = sorted(metric_list)[round(runs*0.005)]\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b196033d-08ea-4d34-a6e9-aa7e42564cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1\n",
      "mean_raw_agreement: {'abs value': 0.7389999999999998, 'bootstrap_sd': 0.010207648798817512, 'bootstrap_ci_995': 0.7662500000000001, 'bootstrap_ci_005': 0.7137499999999997}\n",
      "fleiss kappa: {'abs value': 0.19762597248876537, 'bootstrap_sd': 0.020369067917837692, 'bootstrap_ci_995': 0.25010525011233514, 'bootstrap_ci_005': 0.1472697246577277}\n",
      "\n",
      "g2\n",
      "mean_raw_agreement: {'abs value': 0.9372499999999997, 'bootstrap_sd': 0.005853340392459645, 'bootstrap_ci_995': 0.9530000000000004, 'bootstrap_ci_005': 0.9219999999999989}\n",
      "fleiss kappa: {'abs value': 0.7780866026522338, 'bootstrap_sd': 0.01951185692610804, 'bootstrap_ci_995': 0.8308421515853782, 'bootstrap_ci_005': 0.726547514064108}\n",
      "\n",
      "g3\n",
      "mean_raw_agreement: {'abs value': 0.7249999999999999, 'bootstrap_sd': 0.008634983899087503, 'bootstrap_ci_995': 0.74625, 'bootstrap_ci_005': 0.7030000000000001}\n",
      "fleiss kappa: {'abs value': 0.1476217606352577, 'bootstrap_sd': 0.020746850245156562, 'bootstrap_ci_995': 0.2090009339654267, 'bootstrap_ci_005': 0.09682458904547743}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runs = 1000\n",
    "\n",
    "for g in [\"g1\",\"g2\",\"g3\"]:\n",
    "    print(g)\n",
    "    results = bootstrap_se(annotations[g], runs = runs)\n",
    "    print(\"mean_raw_agreement:\", results[\"mean_raw_agreement\"])\n",
    "    print(\"fleiss kappa:\", results[\"fleiss_kappa\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2850f-03ab-4813-a6ce-5b9b73685829",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute bootstrap CIs for difference between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2f43e5-3829-41c2-98bd-472c27d9cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_bootstrap_se(df1, df2, runs):\n",
    "    \n",
    "    results_dict={}\n",
    "    \n",
    "    # DIFF IN MEANS\n",
    "    \n",
    "    # RAW AGREEMENT\n",
    "    results_dict['diff_mean_raw_agreement']={}\n",
    "    results_dict['diff_mean_raw_agreement']['abs value'] = df1['pct_agreement'].mean()-df2['pct_agreement'].mean()\n",
    "    \n",
    "    # FLEISS KAPPA\n",
    "    results_dict['diff_fleiss_kappa']={}\n",
    "    results_dict['diff_fleiss_kappa']['abs value'] = fleiss_kappa(df1[['n_hateful','n_nonhateful']].to_numpy())-fleiss_kappa(df2[['n_hateful','n_nonhateful']].to_numpy())\n",
    "    \n",
    "    \n",
    "    # BOOTSTRAP\n",
    "    \n",
    "    metric_list = []\n",
    "    for i in range(runs):\n",
    "        b_df1 = df1.sample(frac=1, replace=True, random_state=i)\n",
    "        b_df2 = df2.sample(frac=1, replace=True, random_state=i)\n",
    "        metric_list.append(b_df1['pct_agreement'].mean()-b_df2['pct_agreement'].mean())\n",
    "    \n",
    "    results_dict['diff_mean_raw_agreement']['bootstrap_sd'] = statistics.pstdev(metric_list)\n",
    "    results_dict['diff_mean_raw_agreement']['bootstrap_ci_995'] = sorted(metric_list)[round(runs*0.995)]\n",
    "    results_dict['diff_mean_raw_agreement']['bootstrap_ci_005'] = sorted(metric_list)[round(runs*0.005)]\n",
    "    \n",
    "    \n",
    "    metric_list = []\n",
    "    for i in range(runs):\n",
    "        b_df1 = df1.sample(frac=1, replace=True, random_state=i)\n",
    "        b_df2 = df2.sample(frac=1, replace=True, random_state=i)\n",
    "        metric_list.append(fleiss_kappa(b_df1[['n_hateful','n_nonhateful']].to_numpy())-fleiss_kappa(b_df2[['n_hateful','n_nonhateful']].to_numpy()))\n",
    "    \n",
    "    results_dict['diff_fleiss_kappa']['bootstrap_sd'] = statistics.pstdev(metric_list)\n",
    "    results_dict['diff_fleiss_kappa']['bootstrap_ci_995'] = sorted(metric_list)[round(runs*0.995)]\n",
    "    results_dict['diff_fleiss_kappa']['bootstrap_ci_005'] = sorted(metric_list)[round(runs*0.005)]\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db3aba0-7060-4a36-b0c2-a4d1593af5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a125ad-a5ff-4680-8479-fb6dab3572ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diff_mean_raw_agreement': {'abs value': -0.19824999999999993,\n",
       "  'bootstrap_sd': 0.011069228563906346,\n",
       "  'bootstrap_ci_995': -0.169249999999999,\n",
       "  'bootstrap_ci_005': -0.22649999999999926},\n",
       " 'diff_fleiss_kappa': {'abs value': -0.5804606301634685,\n",
       "  'bootstrap_sd': 0.02444507553644793,\n",
       "  'bootstrap_ci_995': -0.5121998744546794,\n",
       "  'bootstrap_ci_005': -0.6417281912716728}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_bootstrap_se(annotations[\"g1\"],annotations[\"g2\"], runs = runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32a736d-d35a-4cc9-ab5e-d71c5c299519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diff_mean_raw_agreement': {'abs value': 0.21224999999999983,\n",
       "  'bootstrap_sd': 0.010280445163026774,\n",
       "  'bootstrap_ci_995': 0.23650000000000027,\n",
       "  'bootstrap_ci_005': 0.18449999999999966},\n",
       " 'diff_fleiss_kappa': {'abs value': 0.6304648420169762,\n",
       "  'bootstrap_sd': 0.024782199760685585,\n",
       "  'bootstrap_ci_995': 0.6936047678263629,\n",
       "  'bootstrap_ci_005': 0.5697894532376929}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_bootstrap_se(annotations[\"g2\"],annotations[\"g3\"], runs = runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2bc5f1-2c90-4719-b697-1b352d5d111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diff_mean_raw_agreement': {'abs value': 0.013999999999999901,\n",
       "  'bootstrap_sd': 0.0081724041597317,\n",
       "  'bootstrap_ci_995': 0.03599999999999948,\n",
       "  'bootstrap_ci_005': -0.006499999999999284},\n",
       " 'diff_fleiss_kappa': {'abs value': 0.050004211853507674,\n",
       "  'bootstrap_sd': 0.015458239712014649,\n",
       "  'bootstrap_ci_995': 0.08974109044085335,\n",
       "  'bootstrap_ci_005': 0.011126210618792848}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_bootstrap_se(annotations[\"g1\"],annotations[\"g3\"], runs = runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
