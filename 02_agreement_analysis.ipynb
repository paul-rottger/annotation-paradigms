{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb5f60c-80e0-4a41-b71c-e38c5d33bb6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AGREEMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db579451-9559-46c4-9339-768d33b1d5be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialise packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226bff1b-7426-485e-80ea-8f20c4a9a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bec635-e271-4ebb-9b82-e9b8624095a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc65da57-1415-4a83-8e55-4dce43d81e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./0_data/clean/davidson2017_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fc222-e077-4336-a26d-34f0bd4e255b",
   "metadata": {},
   "source": [
    "### Compute inter-annotator agreement metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3a0a4-cdab-4d2d-b54b-691bbd1367e5",
   "metadata": {},
   "source": [
    "**Percentage agreement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975e657a-ef0c-4bff-a4df-73cb77b538eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage agreement:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9051899270691972"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all data is annotated by three annotators, so agreement is either 2/3 or 3/3\n",
    "# this information is already saved in the 'unanimous' column\n",
    "\n",
    "df['pct_agreement'] = df.unanimous.apply(lambda x: 2/3 if x==False else 1)\n",
    "\n",
    "print('Average percentage agreement:')\n",
    "df.pct_agreement.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e93f5c-a1ef-45ce-b9c0-e23da775c098",
   "metadata": {},
   "source": [
    "**Fleiss' Kappa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d194069-7adc-4d88-b11e-76d0d676c9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5494695584402823"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote_matrix = df[['n_hateful','n_offensive', 'n_neither']].to_numpy()\n",
    "\n",
    "print('Fleiss\\' Kappa:')\n",
    "fleiss_kappa(vote_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cfb9aa-895d-47f4-b565-3545ae74e95c",
   "metadata": {},
   "source": [
    "### Compute bootstrap standard errors for agreement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cd8423-bce2-4bd1-93fa-b07a9c495fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_se(df, runs):\n",
    "    \n",
    "    results_dict={}\n",
    "    \n",
    "    # MEANS\n",
    "    \n",
    "    # RAW AGREEMENT\n",
    "    results_dict['mean_raw_agreement']={}\n",
    "    results_dict['mean_raw_agreement']['abs value'] = df['pct_agreement'].mean()\n",
    "    \n",
    "    # FLEISS KAPPA\n",
    "    results_dict['fleiss_kappa']={}\n",
    "    results_dict['fleiss_kappa']['abs value'] = fleiss_kappa(df[['n_hateful','n_offensive', 'n_neither']].to_numpy())\n",
    "    \n",
    "    \n",
    "    # BOOTSTRAP\n",
    "    \n",
    "    metric_list = []\n",
    "    for i in range(runs):\n",
    "        b_df = df.sample(frac=1, replace=True)\n",
    "        metric_list.append(b_df['pct_agreement'].mean())\n",
    "    \n",
    "    results_dict['mean_raw_agreement']['bootstrap_sd'] = statistics.pstdev(metric_list)\n",
    "    \n",
    "    metric_list = []\n",
    "    for i in range(runs):\n",
    "        b_df = df.sample(frac=1, replace=True)\n",
    "        metric_list.append(fleiss_kappa(b_df[['n_hateful','n_offensive', 'n_neither']].to_numpy()))\n",
    "    \n",
    "    results_dict['fleiss_kappa']['bootstrap_sd'] = statistics.pstdev(metric_list)\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b196033d-08ea-4d34-a6e9-aa7e42564cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_raw_agreement': {'abs value': 0.9036666666666736,\n",
       "  'bootstrap_sd': 0.0033223808564874576},\n",
       " 'fleiss_kappa': {'abs value': 0.542276288525904,\n",
       "  'bootstrap_sd': 0.014908332137299578}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2000\n",
    "runs = 500\n",
    "\n",
    "bootstrap_se(df.sample(n, random_state=123), runs = runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
